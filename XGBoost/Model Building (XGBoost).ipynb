{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lv6gmmaGWlg2"
   },
   "outputs": [],
   "source": [
    "import Preprocessing_Function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install if needed\n",
    "\n",
    "#!python -m spacy download en_core_web_md  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMBNY9FHjMXN"
   },
   "outputs": [],
   "source": [
    "start = pd.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata = Preprocessing_Function.import_data()\n",
    "data_label = Preprocessing_Function.labelled_data(fulldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_koactdGW1Mk"
   },
   "outputs": [],
   "source": [
    "data = fulldata # full data\n",
    "training = data_label # data with labels\n",
    "data1 = data.copy() # copy\n",
    "training1 = training.copy() # copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy4k7hvDZ9uw"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records with missing values for X-s and specified y\n",
    "def getclean(df: pd.DataFrame(), y: str()):\n",
    "    temp = df.copy()\n",
    "    # drop redundant index columns\n",
    "    temp.drop(['index'], axis = 1, inplace = True)\n",
    "    # remove records with missing data for X-s and y (here - category)\n",
    "    temp.dropna(subset=['brand','description','details',y], inplace = True)\n",
    "    temp.reset_index(inplace = True)\n",
    "    temp.drop('index', axis = 1, inplace = True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize descriptions in df using doc2vecs trained on fulL_data descriptions\n",
    "def doc2vecdesc(df: pd.DataFrame()):\n",
    "    temp = data1.copy()\n",
    "    temp.dropna(subset = ['description'], inplace = True)\n",
    "    temp.reset_index(inplace = True)\n",
    "    temp.drop(['index'], axis = 1, inplace = True)\n",
    "    # retrieve description documents\n",
    "    documents = [TaggedDocument(str(temp.loc[i,'description']), [i]) for i in temp.index]\n",
    "    # train doc2vec on full data descriptions and vectorize training documents\n",
    "    model = Doc2Vec(documents, vector_size=300, window=3, workers=4)\n",
    "    df['VectorizedDesc'] = df.description.apply(lambda x: model.infer_vector([x]))\n",
    "    description_vectors = list(df.VectorizedDesc.values)\n",
    "    return description_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBTRnUAbTZSX"
   },
   "outputs": [],
   "source": [
    "# tf-idf doc vectorization for names and details\n",
    "def tfidfvectorize(x: list()):\n",
    "    # tf-idf vectorize on all unique tokens in list 'x' and sum the scores for each doc\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(x)\n",
    "    tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "    tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "    # create a list of all tokens extracted by tf-idf\n",
    "    available_tf_idf_scores = tf_idf_lookup_table.columns\n",
    "    available_tf_idf_scores = list(map(lambda x: x, available_tf_idf_scores))\n",
    "    # define an empty list\n",
    "    docvectors = []\n",
    "    # obtain tf-idf doc embeddings for every element in 'x'\n",
    "    for idx, x in enumerate(x): \n",
    "        tokens = nlp(x)\n",
    "        total_tf_idf_score_per_document = 0\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens:\n",
    "            if token.has_vector and token.text in available_tf_idf_scores:\n",
    "                tf_idf_score = tf_idf_lookup_table.loc[idx, token.text]\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "        document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        docvectors.append(document_embedding)\n",
    "    # return the embeddings\n",
    "    return docvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to perform classification for single-label targets (using softmax as our 'activation')\n",
    "# preparing the data for xgboost, which is then run in this function \n",
    "# takes about 5-7 minutes to run with 100 trees and 5 folds\n",
    "def softmax(df: pd.DataFrame(), description_vectors: list(), names_vectors: list(), details_vectors: list(),\\\n",
    "            attribute: str(), fold: int(), seed: int()):\n",
    "    brandsdf = pd.get_dummies(df.brand)\n",
    "    namesdf = pd.DataFrame(np.array(names_vectors))\n",
    "    detailsdf = pd.DataFrame(np.array(details_vectors))\n",
    "    descriptiondf = pd.DataFrame(np.array(description_vectors))\n",
    "    X = pd.concat([brandsdf,namesdf,detailsdf,descriptiondf], axis = 1) # combine embeddings into a data frame\n",
    "    X.columns = [f'col{i}' for i in range(X.shape[1])]\n",
    "    d = {}\n",
    "    for i in range(len(df[attribute].unique())):\n",
    "        d.update({df[attribute].unique()[i]: i}) # create a dicionary where a unique attribute from class is stored as key \n",
    "                                                 # and is assigned a unique index as value\n",
    "    y = df[attribute].map(d) # map keys to values\n",
    "    kf = KFold(n_splits = fold, random_state = seed, shuffle = True) # define the k-fold constructor\n",
    "    temp = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        # define the constructor, where hyperparameters were tuned using GridSearchCV on 1350 different combinations of \n",
    "        # parameters ['max_depth','n_estimators','reg_lambda','learning_rate','colsample_bytree']\n",
    "        clf = xgb.XGBClassifier(max_depth=8, objective='multi:softprob', n_estimators=100, reg_lambda=0.25,\\\n",
    "                         learning_rate=0.1,colsample_bytree=0.25,num_classes=len(y.unique()))\n",
    "        clf.fit(X_train, y_train)  \n",
    "        pred = clf.predict(X_test)\n",
    "        temp.append(np.mean(pred != y_test)) # misclassification rate\n",
    "    return f'{attribute} accuracy: {(1-np.mean(temp))*100}' # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to perform classification for multi-label targets (using sigmoid as our 'activation')\n",
    "# preparing the data for xgboost, which is then run in this function\n",
    "# takes about 5-7 minutes to run with 100 trees and 5 folds\n",
    "def sigmoid(df: pd.DataFrame(), description_vectors: list(), names_vectors: list(), details_vectors: list(),\\\n",
    "            attribute: str(), fold: int(), seed: int()):\n",
    "    brandsdf = pd.get_dummies(df.brand)\n",
    "    namesdf = pd.DataFrame(np.array(names_vectors))\n",
    "    detailsdf = pd.DataFrame(np.array(details_vectors))\n",
    "    descriptiondf = pd.DataFrame(np.array(description_vectors))\n",
    "    X = pd.concat([brandsdf,namesdf,detailsdf,descriptiondf], axis = 1) # combine embeddings into a data frame\n",
    "    X.columns = [f'col{i}' for i in range(X.shape[1])]\n",
    "    l = []\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in df.loc[i,attribute].split(' '):\n",
    "            if j not in l:\n",
    "                l.append(j) # retrieve all unique attributes from class and store them in a list\n",
    "    results = []\n",
    "    for subattr in l: # iterate xgboost over each unique attribute - resulting in a separate model for every attribute\n",
    "        y = df[attribute].apply(lambda x: 1 if subattr in x.split(' ') else 0)\n",
    "        kf = KFold(n_splits = fold, random_state = seed, shuffle = True)\n",
    "        temp = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            # define the constructor, where hyperparameters were tuned using GridSearchCV on 1350 different combinations of \n",
    "            # parameters ['max_depth','n_estimators','reg_lambda','learning_rate','colsample_bytree']\n",
    "            clf = xgb.XGBClassifier(max_depth=8, objective='binary:logistic', n_estimators=100, reg_lambda=0.25,\\\n",
    "                             learning_rate=0.1,colsample_bytree=0.25,num_classes=len(y.unique()))\n",
    "            clf.fit(X_train, y_train)  \n",
    "            pred = clf.predict(X_test)\n",
    "            temp.append(np.mean(pred != y_test)) # misclassification rate\n",
    "        results.append(f'{attribute}_{subattr} accuracy: {(1-np.mean(temp))*100}') # accuracy\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for 'Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3321"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = getclean(training1, 'category')\n",
    "training.shape[0] # 3321 records where X-s and y (category) are not NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors = doc2vecdesc(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_vectors = list(pd.get_dummies(training.brand).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcZ2Ad2MRdjQ"
   },
   "outputs": [],
   "source": [
    "names = list(training.name.values)\n",
    "details = list(training.details.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-gtlBA-WXWV"
   },
   "outputs": [],
   "source": [
    "names_vectors = tfidfvectorize(names)\n",
    "details_vectors = tfidfvectorize(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vIQaDBrKWkyT",
    "outputId": "0f47eaa3-9424-4145-9a9a-04b63eb8efb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = [len(description_vectors)-training.shape[0],len(brand_vectors)-training.shape[0],\\\n",
    "         len(names_vectors)-training.shape[0], len(details_vectors)-training.shape[0]]\n",
    "not any(check) # 'True' here indicates that all embedded objects have the same number of documents as does the 'training' df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'category accuracy: 82.53538090936465'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(training,description_vectors,names_vectors,details_vectors,'category',3,2) # using 3 folds and random.seed = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(df: pd.DataFrame(), fold: int(), seed: int()):\n",
    "    results = []\n",
    "    for i in ['category','fit','style','occasion']: # iterate through each selected class to predict\n",
    "        training = getclean(df, i) # cleaned data (accounting for NA-s) according to the selected class\n",
    "        description_vectors = doc2vecdesc(training) # description embeddings\n",
    "        brand_vectors = list(pd.get_dummies(training.brand).values) # one-hot encoded brands\n",
    "        names = list(training.name.values) # product names\n",
    "        details = list(training.details.values) # product details\n",
    "        names_vectors = tfidfvectorize(names) # product name embeddings\n",
    "        details_vectors = tfidfvectorize(details) # product details embeddings\n",
    "        check = [len(description_vectors)-training.shape[0],len(brand_vectors)-training.shape[0],\\\n",
    "                 len(names_vectors)-training.shape[0], len(details_vectors)-training.shape[0]] \n",
    "        # any(check) == 'False' here indicates that all embedded objects have the same number of documents \n",
    "        # as does the 'training' df\n",
    "        if any(check):\n",
    "            return f'Error: Number of embedded rows does not match the number of labeled records for class {i}'\n",
    "        if i == 'category' or i == 'fit':\n",
    "            results.append(softmax(training,description_vectors,names_vectors,details_vectors,i,fold,seed)) # single-label\n",
    "        else:\n",
    "            results.append(sigmoid(training,description_vectors,names_vectors,details_vectors,i,fold,seed)) # multi-label\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category accuracy: 83.16695352839932',\n",
       " 'fit accuracy: 51.19028907986244',\n",
       " ['style_glam accuracy: 88.60606060606061',\n",
       "  'style_businesscasual accuracy: 78.48484848484848',\n",
       "  'style_classic accuracy: 68.75757575757576',\n",
       "  'style_modern accuracy: 70.33333333333334',\n",
       "  'style_romantic accuracy: 86.81818181818181',\n",
       "  'style_casual accuracy: 74.57575757575758',\n",
       "  'style_edgy accuracy: 79.78787878787878',\n",
       "  'style_retro accuracy: 94.03030303030303',\n",
       "  'style_androgynous accuracy: 84.57575757575756',\n",
       "  'style_athleisure accuracy: 96.36363636363636',\n",
       "  'style_boho accuracy: 88.0909090909091'],\n",
       " ['occasion_weekend accuracy: 75.88690230596433',\n",
       "  'occasion_work accuracy: 76.46220143950855',\n",
       "  'occasion_daytonight accuracy: 76.55205611332691',\n",
       "  'occasion_nightout accuracy: 77.79525970751386',\n",
       "  'occasion_vacation accuracy: 86.12483381469765',\n",
       "  'occasion_workout accuracy: 97.27369000137533',\n",
       "  'occasion_coldweather accuracy: 94.0625773621235']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine(training1,5,0) # 5+ fold CV-s are encouraged, but take significantly more time to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EQy3-lbjMab"
   },
   "outputs": [],
   "source": [
    "end = pd.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rq__rhc-jMaf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:44:43.902164\n"
     ]
    }
   ],
   "source": [
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Model1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
