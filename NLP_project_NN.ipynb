{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP_project_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgJTTUmQFuLX",
        "colab_type": "code",
        "outputId": "ab221509-d28d-438d-9d9d-179ba67e3255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8WfAo2FYfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u11F0aeSFYfd",
        "colab_type": "code",
        "outputId": "3aa6800e-796f-492f-b664-cb5fd07c68d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "start = pd.datetime.now()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_60huqjFYfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/NLP/df_clean.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EBVEK_fKFYft",
        "colab_type": "code",
        "outputId": "c3d9a1f1-e1fe-48be-af15-b598672aa86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>product_id</th>\n",
              "      <th>brand</th>\n",
              "      <th>mpn</th>\n",
              "      <th>product_full_name</th>\n",
              "      <th>description</th>\n",
              "      <th>brand_category</th>\n",
              "      <th>brand_canonical_url</th>\n",
              "      <th>details</th>\n",
              "      <th>labels</th>\n",
              "      <th>attribute_name</th>\n",
              "      <th>attribute_value</th>\n",
              "      <th>style</th>\n",
              "      <th>occasion</th>\n",
              "      <th>category</th>\n",
              "      <th>fit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>01e5zxp5h0btezt9qd2hrzj47a</td>\n",
              "      <td>a.l.c.</td>\n",
              "      <td>5529544</td>\n",
              "      <td>lennox high waist cotton linen pant</td>\n",
              "      <td>high rise bottom tailored cool italian cotton ...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://shop.nordstrom.com/s/a-l-c-lennox-high...</td>\n",
              "      <td>true size high rise inseam leg opening rise ri...</td>\n",
              "      <td>[]</td>\n",
              "      <td>style</td>\n",
              "      <td>modern</td>\n",
              "      <td>modern</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>01e5zxp5h0btezt9qd2hrzj47a</td>\n",
              "      <td>a.l.c.</td>\n",
              "      <td>5529544</td>\n",
              "      <td>lennox high waist cotton linen pant</td>\n",
              "      <td>high rise bottom tailored cool italian cotton ...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://shop.nordstrom.com/s/a-l-c-lennox-high...</td>\n",
              "      <td>true size high rise inseam leg opening rise ri...</td>\n",
              "      <td>[]</td>\n",
              "      <td>style</td>\n",
              "      <td>businesscasual</td>\n",
              "      <td>businesscasual</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>01e5zxp5h0btezt9qd2hrzj47a</td>\n",
              "      <td>a.l.c.</td>\n",
              "      <td>5529544</td>\n",
              "      <td>lennox high waist cotton linen pant</td>\n",
              "      <td>high rise bottom tailored cool italian cotton ...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://shop.nordstrom.com/s/a-l-c-lennox-high...</td>\n",
              "      <td>true size high rise inseam leg opening rise ri...</td>\n",
              "      <td>[]</td>\n",
              "      <td>style</td>\n",
              "      <td>classic</td>\n",
              "      <td>classic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>01e5zxp5h0btezt9qd2hrzj47a</td>\n",
              "      <td>a.l.c.</td>\n",
              "      <td>5529544</td>\n",
              "      <td>lennox high waist cotton linen pant</td>\n",
              "      <td>high rise bottom tailored cool italian cotton ...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://shop.nordstrom.com/s/a-l-c-lennox-high...</td>\n",
              "      <td>true size high rise inseam leg opening rise ri...</td>\n",
              "      <td>[]</td>\n",
              "      <td>occasion</td>\n",
              "      <td>work</td>\n",
              "      <td>NaN</td>\n",
              "      <td>work</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>01e5zxp5h0btezt9qd2hrzj47a</td>\n",
              "      <td>a.l.c.</td>\n",
              "      <td>5529544</td>\n",
              "      <td>lennox high waist cotton linen pant</td>\n",
              "      <td>high rise bottom tailored cool italian cotton ...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://shop.nordstrom.com/s/a-l-c-lennox-high...</td>\n",
              "      <td>true size high rise inseam leg opening rise ri...</td>\n",
              "      <td>[]</td>\n",
              "      <td>category</td>\n",
              "      <td>bottom</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bottom</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                  product_id   brand  ... occasion category  fit\n",
              "0      0  01e5zxp5h0btezt9qd2hrzj47a  a.l.c.  ...      NaN      NaN  NaN\n",
              "1      1  01e5zxp5h0btezt9qd2hrzj47a  a.l.c.  ...      NaN      NaN  NaN\n",
              "2      2  01e5zxp5h0btezt9qd2hrzj47a  a.l.c.  ...      NaN      NaN  NaN\n",
              "3      3  01e5zxp5h0btezt9qd2hrzj47a  a.l.c.  ...     work      NaN  NaN\n",
              "4      4  01e5zxp5h0btezt9qd2hrzj47a  a.l.c.  ...      NaN   bottom  NaN\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PovvCwVbZfH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "db7c8279-2ce7-459d-87d3-9b1f8991049d"
      },
      "source": [
        "df['category'].unique()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, 'bottom', 'top', 'sweater', 'onepiece', 'blazerscoatsjackets',\n",
              "       'shoe', 'accessory', 'sweatshirthoodie'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Cwfg0zZfOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['category'] = np.where(df['category']=='sweater', 'top', np.where(df['category']=='blazerscoatsjackets', 'top', np.where(df['category']=='sweatshirthoodie', 'top', df['category'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmNFu__ibTP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4db34c22-3c16-4e26-c53a-7bb12cdf6171"
      },
      "source": [
        "df['occasion'].unique()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, 'work', 'daytonight', 'weekend', 'vacation', 'nightout',\n",
              "       'coldweather', 'workout'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fem7N_g1FYfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['mpn','brand_canonical_url','labels'], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA0cUjEjFYf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['brand','product_full_name','description','brand_category','details']\n",
        "for col in cols:\n",
        "    df[col].fillna('',inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b-71d3DFYf8",
        "colab_type": "code",
        "outputId": "0b92ca31-a863-4eb6-aabd-efb8aac15e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                    0\n",
              "product_id               0\n",
              "brand                    0\n",
              "product_full_name        0\n",
              "description              0\n",
              "brand_category           0\n",
              "details                  0\n",
              "attribute_name           0\n",
              "attribute_value          0\n",
              "style                16070\n",
              "occasion             17530\n",
              "category             22608\n",
              "fit                  23544\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSTxlReUFYgf",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSPsAAiAFYgg",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zxqpFRyFYgh",
        "colab_type": "code",
        "outputId": "d0c13cbc-3578-4d65-8652-b35a40d78ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_WkQG51FYgl",
        "colab_type": "code",
        "outputId": "8ba56e62-5f52-47be-91e6-4517a16c8468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cols = ['brand','product_full_name','description','brand_category','details']\n",
        "embedding_size=100\n",
        "docs=[]\n",
        "for col in cols:\n",
        "    docs += [word_tokenize(i) for i in df[col]] \n",
        "wordvec = Word2Vec(docs, size=embedding_size, min_count=1)\n",
        "print(wordvec)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=6813, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKfS8QbcGccR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = list(wordvec.wv.vocab)\n",
        "wordvec_num = [wordvec.wv[i] for i in vocab]\n",
        "wordvec_dict = dict(zip(vocab, wordvec_num))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21hxXW7jIdZA",
        "colab_type": "text"
      },
      "source": [
        "### Combining features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u3VUfc7IdTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['vars'] = df['brand']+' '+df['product_full_name']+' '+df['description']+' '+df['brand_category']+' '+df['details']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3lADDmzFYgy",
        "colab_type": "text"
      },
      "source": [
        "# Deep NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otp4H4daFYg2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b14785d-c94f-457c-d2cf-de1401068def"
      },
      "source": [
        "from random import randint\n",
        "from numpy import array, argmax, asarray, zeros\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.layers import Flatten, Masking\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, History\n",
        "import tensorflow as tf"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DZXbTb_Oik9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deepnn_multiclass(X, y, vocab_size, embedding_size, embedding_matrix, max_sequence_len, node=10, val_data:tuple=None, val_split=0):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_sequence_len, trainable=False))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(node, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
        "    model.add(Dense(len(y[0]), activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    #model.summary()\n",
        "    history = model.fit(X, y, validation_data=val_data, validation_split=val_split, epochs=30, verbose=0, callbacks= [EarlyStopping(patience = 5), ModelCheckpoint(filepath = \"/content/drive/My Drive/NLP/weights.hdf5\", save_best_only= True), History()])\n",
        "    model_new = tf.keras.models.load_model(filepath = \"/content/drive/My Drive/NLP/weights.hdf5\")\n",
        "    return model_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQRgxl9iFdT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelcv_multiclass(labelname, n_splits=5):\n",
        "    data = df.dropna(subset=[labelname])\n",
        "    tokenizer = Tokenizer(num_words=None, oov_token=\"UNKNOWN_TOKEN\")\n",
        "    tokenizer.fit_on_texts(data['vars'])\n",
        "    encoded_docs = tokenizer.texts_to_sequences(data['vars'])   \n",
        "\n",
        "    max_sequence_len = 0\n",
        "    for i in encoded_docs:\n",
        "        if len(i) > max_sequence_len:\n",
        "            max_sequence_len=len(i)        \n",
        "\n",
        "    padded_docs = pad_sequences(encoded_docs, maxlen=max_sequence_len, padding='post')\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    labels = to_categorical(encoder.fit_transform(data[labelname]))\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index)+1\n",
        "    embedding_matrix = zeros((vocab_size, embedding_size))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = wordvec_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits, shuffle = True)\n",
        "    kf.get_n_splits(padded_docs)\n",
        "\n",
        "    maximum, node_final, train_acc_final, test_acc_final = 0,0,0,0\n",
        "    for node in [10,20,30]:\n",
        "        train_acc = 0\n",
        "        test_acc = 0\n",
        "        for train_index, test_index in kf.split(padded_docs):\n",
        "            X_train, X_test, y_train, y_test = padded_docs[train_index], padded_docs[test_index], labels[train_index], labels[test_index]\n",
        "            model = deepnn_multiclass(X_train, y_train, vocab_size, embedding_size, embedding_matrix, max_sequence_len, node=node, val_data=(X_test,y_test))\n",
        "            loss_train, accuracy_train = model.evaluate(X_train, y_train, verbose=0)\n",
        "            loss_test, accuracy_test = model.evaluate(X_test, y_test, verbose=0)\n",
        "            train_acc += accuracy_train\n",
        "            test_acc += accuracy_test\n",
        "        if maximum < test_acc/n_splits:\n",
        "            maximum = test_acc/n_splits\n",
        "            node_final = node\n",
        "            train_acc_final = train_acc/n_splits\n",
        "            test_acc_final = test_acc/n_splits\n",
        "          \n",
        "    return node_final, train_acc_final, test_acc_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzJQSM58LMui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deepnn_binary(X, y, vocab_size, embedding_size, embedding_matrix, max_sequence_len, node=10, val_data:tuple=None, val_split=0):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_sequence_len, trainable=False))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(node, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    #model.summary()\n",
        "    history = model.fit(X, y, validation_data=val_data, validation_split=val_split, epochs=30, verbose=0, callbacks= [EarlyStopping(patience = 5), ModelCheckpoint(filepath = \"/content/drive/My Drive/NLP/weights.hdf5\", save_best_only= True), History()])\n",
        "    model_new = tf.keras.models.load_model(filepath = \"/content/drive/My Drive/NLP/weights.hdf5\")\n",
        "    return model_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2acmwAeILAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelcv_binary(labelname, uniquename, n_splits=5):\n",
        "    data = df.dropna(subset=[labelname]).copy()\n",
        "    data['temp'] = np.where(data[labelname]==uniquename, 0, 1)\n",
        "    data.sort_values(by=['product_id','temp'], inplace=True)\n",
        "    data.drop_duplicates(subset=['product_id'], keep='first', inplace=True)\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=None, oov_token=\"UNKNOWN_TOKEN\")\n",
        "    tokenizer.fit_on_texts(data['vars'])\n",
        "    encoded_docs = tokenizer.texts_to_sequences(data['vars'])\n",
        "\n",
        "    max_sequence_len = 0\n",
        "    for i in encoded_docs:\n",
        "        if len(i) > max_sequence_len:\n",
        "            max_sequence_len=len(i)        \n",
        "\n",
        "    padded_docs = pad_sequences(encoded_docs, maxlen=max_sequence_len, padding='post')\n",
        "\n",
        "    labels = np.where(data[labelname]==uniquename,1,0)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index)+1\n",
        "    embedding_matrix = zeros((vocab_size, embedding_size))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = wordvec_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle = True)\n",
        "    kf.get_n_splits(padded_docs)\n",
        "\n",
        "    maximum, node_final, train_acc_final, test_acc_final = 0,0,0,0\n",
        "    for node in [10,20,30]:\n",
        "        train_acc = 0\n",
        "        test_acc = 0\n",
        "        for train_index, test_index in kf.split(padded_docs):\n",
        "            X_train, X_test, y_train, y_test = padded_docs[train_index], padded_docs[test_index], labels[train_index], labels[test_index]\n",
        "            model = deepnn_binary(X_train, y_train, vocab_size, embedding_size, embedding_matrix, max_sequence_len, node=node, val_data=(X_test,y_test))\n",
        "            loss_train, accuracy_train = model.evaluate(X_train, y_train, verbose=0)\n",
        "            loss_test, accuracy_test = model.evaluate(X_test, y_test, verbose=0)\n",
        "            train_acc += accuracy_train\n",
        "            test_acc += accuracy_test\n",
        "        if maximum < test_acc/n_splits:\n",
        "            maximum = test_acc/n_splits\n",
        "            node_final = node\n",
        "            train_acc_final = train_acc/n_splits\n",
        "            test_acc_final = test_acc/n_splits\n",
        "          \n",
        "    return node_final, train_acc_final, test_acc_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD9eVC4_PUhF",
        "colab_type": "code",
        "outputId": "9a66abac-8ce6-4d4c-e386-51454a0a3489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for col in ['category','fit']:\n",
        "    print(f'{col} best node and accuracy (train, test): {modelcv_multiclass(col)}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category best node and accuracy (train, test): (30, 0.9967932581901551, 0.9376280665397644)\n",
            "fit best node and accuracy (train, test): (10, 0.6078947424888611, 0.49572367668151857)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2VFxLCSIo2Z",
        "colab_type": "code",
        "outputId": "430146ae-3c89-46f9-f38a-f9065e6e2032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for col in ['style','occasion']:\n",
        "    for name in df.dropna(subset=[col])[col].unique():\n",
        "        print(f'{col}-{name} best node and accuracy (train, test): {modelcv_binary(col, name)}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "style-modern best node and accuracy (train, test): (20, 0.7823657870292664, 0.6958655834197998)\n",
            "style-businesscasual best node and accuracy (train, test): (30, 0.8548249006271362, 0.7586822628974914)\n",
            "style-classic best node and accuracy (train, test): (20, 0.7905983209609986, 0.6603679656982422)\n",
            "style-casual best node and accuracy (train, test): (30, 0.9023878455162049, 0.755107581615448)\n",
            "style-androgynous best node and accuracy (train, test): (30, 0.864019775390625, 0.8255874156951905)\n",
            "style-boho best node and accuracy (train, test): (30, 0.896067452430725, 0.8804900765419006)\n",
            "style-retro best node and accuracy (train, test): (20, 0.947906231880188, 0.9433119535446167)\n",
            "style-edgy best node and accuracy (train, test): (20, 0.8313317775726319, 0.7990274786949157)\n",
            "style-glam best node and accuracy (train, test): (20, 0.9069833278656005, 0.8919820427894593)\n",
            "style-romantic best node and accuracy (train, test): (10, 0.8739781618118286, 0.8592915058135986)\n",
            "style-athleisure best node and accuracy (train, test): (10, 0.9893389225006104, 0.9537786483764649)\n",
            "occasion-work best node and accuracy (train, test): (20, 0.8583910703659058, 0.7434854507446289)\n",
            "occasion-daytonight best node and accuracy (train, test): (20, 0.8056991577148438, 0.7478277087211609)\n",
            "occasion-weekend best node and accuracy (train, test): (20, 0.8291413426399231, 0.7657145261764526)\n",
            "occasion-vacation best node and accuracy (train, test): (10, 0.8792806744575501, 0.8403177499771118)\n",
            "occasion-nightout best node and accuracy (train, test): (10, 0.8051853775978088, 0.7736259460449219)\n",
            "occasion-coldweather best node and accuracy (train, test): (20, 0.9573332071304321, 0.936637568473816)\n",
            "occasion-workout best node and accuracy (train, test): (10, 0.9830742120742798, 0.966275668144226)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS2zDSK3FYhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4716d973-5409-4958-d50f-8c1a3700ee39"
      },
      "source": [
        "end = pd.datetime.now()\n",
        "print(end-start)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:15:44.972571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}